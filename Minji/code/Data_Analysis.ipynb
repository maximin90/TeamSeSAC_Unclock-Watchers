{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## FOMC, Interview, Speech"
      ],
      "metadata": {
        "id": "twCo-t0FzSpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import file and merge\n",
        "#### interview, fomc, speech"
      ],
      "metadata": {
        "id": "RFAG9iIdzV2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xtZVGzsQzYxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2G3KAoYy95R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 주어진 텍스트 파일 경로\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Unclock-watchers/merge/인터뷰+연설.txt'\n",
        "\n",
        "# 각 항목을 담을 리스트\n",
        "data_list1 = []\n",
        "\n",
        "# 텍스트 파일을 읽어서 각 항목의 시작 지점을 찾아 리스트에 추가\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "    for line in lines:\n",
        "        # 예시로 각 행이 개별 문장 또는 단락으로 구분되어 있다고 가정하고 '\\n'로 나누기\n",
        "        parts = line.strip().split('\\n')\n",
        "\n",
        "        # 리스트에 추가\n",
        "        data_list1.extend(parts)\n",
        "\n",
        "# 데이터프레임으로 변환\n",
        "df1 = pd.DataFrame(data_list1, columns=['Text'])\n",
        "df1['Text'] = df1['Text'].str.replace('_', '')\n",
        "df1['Text'] = df1['Text'].str.replace('--', '')\n",
        "\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 주어진 텍스트 파일 경로\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Unclock-watchers/merge/FOMC_2018~2023.txt'\n",
        "\n",
        "# 각 항목을 담을 리스트\n",
        "data_list2 = []\n",
        "\n",
        "# 텍스트 파일을 읽어서 각 항목의 시작 지점을 찾아 리스트에 추가\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "    for line in lines:\n",
        "        # 예시로 각 행이 개별 문장 또는 단락으로 구분되어 있다고 가정하고 '\\n'로 나누기\n",
        "        parts = line.strip().split('\\n')\n",
        "\n",
        "        # 리스트에 추가\n",
        "        data_list2.extend(parts)\n",
        "\n",
        "# 데이터프레임으로 변환\n",
        "df2 = pd.DataFrame(data_list2, columns=['Text'])\n",
        "df2 = df2.replace('', pd.NA)\n",
        "df2.dropna(inplace = True)"
      ],
      "metadata": {
        "id": "NePZtX9UzdfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '_' 없애기\n",
        "df2['Text'] = df2['Text'].str.replace('_', '')\n",
        "df2['Text'] = df2['Text'].str.replace('--', '')\n",
        "df2['Text'] = df2['Text'].str.replace('\"', '')\n",
        "\n",
        "# 결과 출력\n",
        "df2.info()"
      ],
      "metadata": {
        "id": "cewy7gFNzeCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(df1, df2, on='Text', how='outer')\n",
        "merged_df"
      ],
      "metadata": {
        "id": "6DHVWC6MzfSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/Unclock-watchers/merge/merged_2018~2023.txt\",index=False, encoding='utf-8-sig',mode ='w')"
      ],
      "metadata": {
        "id": "XbPptMOyzgbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data cleaning"
      ],
      "metadata": {
        "id": "GmK1s5eIzigM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 주어진 텍스트 파일 경로\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Unclock-watchers/merge/merged_2018~2023.txt'\n",
        "\n",
        "# 각 항목을 담을 리스트\n",
        "data_list = []\n",
        "\n",
        "# 텍스트 파일을 읽어서 각 항목의 시작 지점을 찾아 리스트에 추가\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "    for line in lines:\n",
        "        # 예시로 각 행이 개별 문장 또는 단락으로 구분되어 있다고 가정하고 '\\n'로 나누기\n",
        "        parts = line.strip().split('\\n')\n",
        "\n",
        "        # 리스트에 추가\n",
        "        data_list.extend(parts)\n",
        "\n",
        "# 데이터프레임으로 변환\n",
        "df = pd.DataFrame(data_list, columns=['Text'])\n",
        "df['Text'] = df['Text'].str.replace('\"', '')\n",
        "df"
      ],
      "metadata": {
        "id": "FW36eXopzi-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # 토큰화 ?\n",
        "nltk.download('stopwords') # 불용어처리\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "JoIVRT0zzkWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contractions(text):\n",
        "    # 축약형 패턴 정의\n",
        "    contractions = {\n",
        "        \"won't\": \"will not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"I'm\" : \"I am\",\n",
        "        \"you're\" : \"you are\",\n",
        "        \"it's\" : \"it is\",\n",
        "        \"didn't\" : \"did not\",\n",
        "        \"don't\" : \"do not\",\n",
        "        \"Don't\" : \"Do not\",\n",
        "        \"doesn't\" : \"does not\",\n",
        "        \"we're\" : \"we are\",\n",
        "        \"shouldn't\" : \"should not\",\n",
        "        \"they're\" : \"they are\",\n",
        "        \"They're\" : \"They are\",\n",
        "        \"haven't\" : \"have not\",\n",
        "        \"should've\" : \"should have\",\n",
        "        \"would've\" : \"would have\",\n",
        "        \"could've\" : \"could have\",\n",
        "        \"there'll\" : \"there will\",\n",
        "        \"It's\" : \"It is\",\n",
        "        \"Where's\" : \"Where is\",\n",
        "        \"that's\" : \"that is\",\n",
        "        \"That's\" : \"That is\",\n",
        "        \"You've\": \"You have\",\n",
        "        \"wouldn't\" : \"would not\",\n",
        "        \"we've\" : \"we have\",\n",
        "        \"They'll\" : \"They will\",\n",
        "        \"they'll\" : \"they will\",\n",
        "        \"I'd\" : \"I would\",\n",
        "        \"i'd\" : \"i would\",\n",
        "        \"There's\": \"There is\",\n",
        "        \"We've\" : \"We have\",\n",
        "        \"you've\" : \"you have\",\n",
        "        \"You've\" : \"You have\",\n",
        "        \"we'll\" : \"we will\",\n",
        "        \"We'll \" : \"We will\",\n",
        "        \"I'll\" : \"I will\",\n",
        "        \"We're\" :\"We are\",\n",
        "        \"there's\" : \"there is\",\n",
        "        \"they'll\" : \"they will\",\n",
        "        \"wasn't\" : \"was not\",\n",
        "        \"I've\" : \"I have\",\n",
        "        \"what's\" : \"what is\",\n",
        "        \"What's\" : \"What is\",\n",
        "        \"let's\" : \"let us\",\n",
        "        \"Let's\" : \"Let us\",\n",
        "        \"they've\" : \"they have\",\n",
        "        \"government's\" : \"government is\",\n",
        "\t      \"inflation's\" : \"inflation is\",\n",
        "        \"Fed's\" : \"Fed is\",\n",
        "        \"isn't\" : \"is not\",\n",
        "        \"it'll\" : \"it will\",\n",
        "        \"It'll\" : \"It will\",\n",
        "        \"economy's\" : \"economy is\",\n",
        "        \"we'd\" : \"we would\",\n",
        "        \"you'd\" : \"you would\",\n",
        "        \"productivity's\" : \"productivity is\",\n",
        "        \"Inflation's\" : \"Inflation is \",\n",
        "        \"You're\" : \"you are\",\n",
        "        \"You'll\" : \"You will\",\n",
        "        \"hasn't\" : \"has not\",\n",
        "        \"who's\" : \"who is\",\n",
        "        \"hasn't\" : \"has not\",\n",
        "        \"he's\" : \"he is\",\n",
        "        \"aren't\" : \"are not\",\n",
        "        \"year's\" : \"year is\",\n",
        "        \"Where's\" : \"Where is\",\n",
        "        \"weren't\" : \"were not\",\n",
        "        \"you'll\" : \"you will\",\n",
        "        \"she's\" : \"she is\",\n",
        "        \"they'd\" : \"they would\",\n",
        "        \"Isn't\" : \"Is not\",\n",
        "        \"something's\" : \"somgthing is\",\n",
        "        \"They've\" : \"They have,\",\n",
        "        \"who've\" : \"who have\",\n",
        "        \"that've\" : \"that have\",\n",
        "        \"We'd\" : \"we would\",\n",
        "        \"who'd\" : \"who had\"\n",
        "\n",
        "    }\n",
        "\n",
        "    # 축약형을 확장\n",
        "    for contraction, expansion in contractions.items():\n",
        "        text = re.sub(contraction, expansion, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "df['Text'] = df['Text'].apply(expand_contractions)\n",
        "df['Text'] = df['Text'].str.replace('``', '')\n",
        "df"
      ],
      "metadata": {
        "id": "xgsZv2VkzlcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불필요한 문자 제거\n",
        "import re\n",
        "\n",
        "# 정규 표현식 패턴 정의 (문자 두 개 이하 및 숫자, 특수문자 제거)\n",
        "combined_pattern = re.compile(r'\\W*\\b(?:\\w{1,2}|[^\\w\\s.: -`]|'')\\b')\n",
        "\n",
        "# 각 단어에 대해 패턴 적용\n",
        "re_df = pd.DataFrame({\n",
        "    'Text': [combined_pattern.sub('', word) for word in df['Text']]\n",
        "})\n",
        "\n",
        "# 결과 확인\n",
        "# print(re_df)\n",
        "\n",
        "# 문장 토큰화\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# 'Text' 열의 각 행에 대해 문장 토큰화 수행\n",
        "sentences = re_df['Text'].apply(lambda x: sent_tokenize(str(x)))\n",
        "\n",
        "# 결과 확인\n",
        "print('문장 토큰화1 :', sentences)"
      ],
      "metadata": {
        "id": "XtU1m8rqzmlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "# 불용어 처리 및 품사 태깅\n",
        "\n",
        "# 만약 하나의 리스트로 유지하고 싶다면, 각 문장의 토큰을 모두 담은 단일 리스트를 만들어서 POS_tagging 함수에 전달하면 됩니다.\n",
        "\n",
        "# stopwords = nltk.corpus.stopwords.words('english') + 마침표 등 삭제?\n",
        "stopwords = nltk.corpus.stopwords.words('english') + list(string.punctuation)\n",
        "tokenized_content = sentences.apply(lambda x: nltk.word_tokenize(str(x)))\n",
        "filtered_tokens = [word.lower() for token_list in tokenized_content for word in token_list if word.lower() not in stopwords]\n",
        "\n",
        "\n",
        "# Print the result -> 너무 많아서 안된다고 뜸\n",
        "# https://whoishoo.tistory.com/546 오류 해결\n",
        "# filtered_tokens\n",
        "\n",
        "def POS_tagging(tokens):\n",
        "    return nltk.pos_tag(tokens)\n",
        "\n",
        "# 모든 토큰에 대해 품사 태깅 수행\n",
        "pos_tags = POS_tagging(filtered_tokens)\n",
        "\n",
        "# 결과 확인\n",
        "pos_tags"
      ],
      "metadata": {
        "id": "NJ6UWSKpzoKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "[lemmatizer.lemmatize(word) for word in filtered_tokens]"
      ],
      "metadata": {
        "id": "DNpz-9ZJzpRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제로 문장의 모든 토큰에 대해 품사 태깅을 수행하려면 각 문장을 하나의 리스트로 유지하는 것이 좋습니다. 만약 하나의 리스트로 유지하지 않고\n",
        "# 여러 개의 리스트로 나눈다면, 문장 간의 토큰 연결이 끊어져서 제대로 된 품사 태깅이 이루어지지 않을 수 있습니다.\n",
        "\n",
        "# 따라서, 문장 간의 경계를 유지하면서 각 문장의 모든 토큰에 대해 품사 태깅을 수행하려면 다음과 같이 코드를 수정할 수 있습니다:\n",
        "\n",
        "# 불용어 및 구두점 제거\n",
        "stopwords = nltk.corpus.stopwords.words('english') + list(string.punctuation)\n",
        "tokenized_content = sentences.apply(lambda x: nltk.word_tokenize(str(x)))\n",
        "filtered_tokens = [word.lower() for token_list in tokenized_content for word in token_list if word.lower() not in stopwords]\n",
        "\n",
        "# 10개의 토큰으로 묶기\n",
        "grouped_tokens = [filtered_tokens[i:i+10] for i in range(0, len(filtered_tokens), 10)]\n",
        "\n",
        "def POS_tagging(token_list):\n",
        "    POS_list = list()\n",
        "\n",
        "    for sentence in token_list:\n",
        "        POS_list.append(nltk.pos_tag(sentence))\n",
        "\n",
        "    return POS_list\n",
        "\n",
        "POS_tagging(grouped_tokens)"
      ],
      "metadata": {
        "id": "oE46ANIbzuEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtered_tokens 대해 토큰화 수행\n",
        "tokenized_content = [nltk.word_tokenize(str(row)) for row in filtered_tokens]\n",
        "\n",
        "# 결과 확인\n",
        "tokenized_content"
      ],
      "metadata": {
        "id": "kgNAAcRLzwau"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}