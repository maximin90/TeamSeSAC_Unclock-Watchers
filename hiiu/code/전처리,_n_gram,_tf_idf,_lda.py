# -*- coding: utf-8 -*-
"""전처리, n-gram, tf-idf, lda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PZbI221Z8hDQziDx6Iuznfy8tJvkhjY6
"""
#nltk 설치
pip install nltk

#불용어 다운로드
nltk.download('punkt')
nltk.download('stopwords')

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import re


def preprocess_text(text):
    # 텍스트 정규화
    text = re.sub(r"I'm", "I am", text)

    # 특수문자 및 숫자 제거
    text = re.sub(r'[^\w\s]', '', text)

    # 소문자로 변환
    text = text.lower()

    # 불용어 제거(나중에 추가하기)
    additional_stopwords = ['year', 'would', 'se', 'iii']
    stop_words = set(stopwords.words('english') + additional_stopwords)
    words = text.split()
    filtered_text = [word for word in words if word not in stop_words]

    # 철자 2개 이하의 단어와 숫자 제거
    filtered_text = [word for word in filtered_text if len(word) > 2 and not word.isdigit()]

    return filtered_text

# 파일 경로
file_path = '/content/drive/MyDrive/Colab Notebooks/fomc 회의록/merged_2018~2023.txt'

# 파일 읽기
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

# 텍스트 전처리 및 토큰화
filtered_text = preprocess_text(text)

# 결과를 데이터프레임으로 변환
df = pd.DataFrame({'Token': filtered_text})

# 데이터프레임 출력
print(df.head)  # 처음 10개의 행 출력

# 결과를 CSV 파일로 저장
df.to_csv('/content/drive/MyDrive/Colab Notebooks/fomc 회의록/filtered_tokens_after_preprocess.csv', index=False)

###5-gram

from nltk.util import ngrams

def generate_ngrams(text, n):
    words = text.split()
    ngrams = zip(*[words[i:] for i in range(n)])
    return [' '.join(ngram) for ngram in ngrams]

# 텍스트 파일 불러오기
file_path = '/content/drive/MyDrive/Colab Notebooks/fomc 회의록/filtered_text5.txt'
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

# n-gram 생성 (예: 5-gram)
five_grams = generate_ngrams(text, 5)

# 결과 출력
print("5-Grams:")
for five_gram in five_grams:
    print(five_gram)

# 저장할 파일 경로
output_file_path_five_grams = '/content/drive/MyDrive/Colab Notebooks/fomc 회의록/five_grams.txt'

# five_grams를 텍스트 파일에 저장
with open(output_file_path_five_grams, 'w', encoding='utf-8') as output_file:
    output_file.write('\n'.join(five_grams))

print(f'5-Grams saved to {output_file_path_five_grams}')

##################################################################################
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# 데이터셋 불러오기 (예: text_emotion.csv)
dataset_path = '/path/to/your/dataset.csv'
df = pd.read_csv(dataset_path)

# 필요한 열만 선택 (text와 emotion 열)
df = df[['text', 'emotion']]

# 학습 및 테스트 데이터셋으로 나누기
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# 전처리 함수 정의
def preprocess_text(text):
    # 텍스트 정규화
    text = text.lower()  # 소문자 변환

    # 여기에 다른 전처리 작업 추가 (특수문자 제거, 불용어 제거 등)

    return text

# 학습 데이터셋에 전처리 적용
train_df['text'] = train_df['text'].apply(preprocess_text)

# TF-IDF 벡터화
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(train_df['text'])
y_train = train_df['emotion']

# Naive Bayes 모델 학습
classifier = MultinomialNB()
classifier.fit(X_train, y_train)

# 테스트 데이터셋에 전처리 적용
test_df['text'] = test_df['text'].apply(preprocess_text)

# TF-IDF 변환
X_test = vectorizer.transform(test_df['text'])
y_test = test_df['emotion']

# 모델 평가
y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f'Accuracy: {accuracy:.2f}')
print('Classification Report:')
print(classification_report(y_test, y_pred))

from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt

# TF-IDF를 사용하여 중요한 단어 식별
corpus = [" ".join(filtered_text)]  # 리스트로 만들어야 함
vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = vectorizer.fit_transform(corpus)

# 각 단어에 대한 중요도 계산
feature_names = vectorizer.get_feature_names_out()
dense = tfidf_matrix.todense()
episode_tfidf = dense.tolist()[0]

# 중요한 단어 출력
important_words = [(word, episode_tfidf[idx]) for idx, word in enumerate(feature_names)]
important_words.sort(key=lambda x: x[1], reverse=True)

print("가장 중요한 단어 20개:")
for word, tfidf_score in important_words[:20]:
    print(f"{word}: {tfidf_score}")


# 중요한 단어 시각화 (막대 그래프)
plt.figure(figsize=(10, 6))
plt.bar([word[0] for word in important_words[:20]],
        [word[1] for word in important_words[:20]],
        color='skyblue')
plt.title('TF-IDF 중요 단어 Top 20')
plt.xlabel('단어')
plt.ylabel('TF-IDF 값')
plt.xticks(rotation=45)
plt.show()

import matplotlib.pyplot as plt
from gensim import corpora
from gensim.models import LdaModel
import matplotlib.pyplot as plt  # 이 줄을 추가하세요

# LDA 모델
dictionary = corpora.Dictionary([filtered_text])
corpus_lda = [dictionary.doc2bow(text) for text in [filtered_text]]
lda_model = LdaModel(corpus_lda, num_topics=10, id2word=dictionary, passes=30)

# 토픽 분포 확인
document_topics = lda_model.get_document_topics(corpus_lda[0])
topic_distribution = [item[1] for item in document_topics]

# 시각화
plt.figure(figsize=(8, 6))
plt.bar(range(len(topic_distribution)), topic_distribution, color='skyblue')
plt.title('토픽 분포')
plt.xlabel('토픽')
plt.ylabel('확률')
plt.show()

# 토픽에 속한 단어 출력
topics = lda_model.print_topics(num_words=10)  # 각 토픽마다 상위 10개의 단어를 출력
for topic in topics:
    print(topic)