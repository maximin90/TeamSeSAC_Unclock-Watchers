# -*- coding: utf-8 -*-
"""the hill 코드.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o95rDhmizZnUH9gR717T6d5GTOtMx39L

#the hill 크롤링 csv파일 저장
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import csv
import os
import pandas as pd
import re

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}


data_list = []

# 625페이지부터 1페이지까지 역순으로 크롤링
for page_number in range(399, 299, -1):
    print(page_number)
    URL = f'https://thehill.com/page/{page_number}/?s=interest+rate&submit=Search&order=desc&orderby=date'
    response = requests.get(URL, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    news = soup.select('div.featured-cards__full__content h1 a')

    # 기사 크롤링
    for i in range(16, 0, -1):
        try:
            current_news_url = news[i].get('href')
            current_news_response = requests.get(current_news_url, headers=headers)
            current_news_soup = BeautifulSoup(current_news_response.text, 'html.parser')

            # 뉴스 제목 추출
            news_title = current_news_soup.select_one('h1').text.strip()

            # 기사 날짜 추출
            date = current_news_soup.find('section', class_='article__header').find('span').text.strip()
            date_string = date
            date_pattern = r'(\d{2}/\d{2}/\d{2})'
            match = re.search(date_pattern, date_string)

            formatted_date = None  # 기본값 설정

            if match:
                extracted_date = match.group(1)
                formatted_date = datetime.strptime(extracted_date, "%m/%d/%y").strftime("%Y-%m-%d")
                print("Date:", formatted_date)

            # 뉴스 기사 내용 추출
            news_content = current_news_soup.select_one('div.article__text').text.strip()

            
            print('')
            if formatted_date:
                print(f"날짜: {formatted_date}")
            print(f"제목: {news_title}")
            print(f"내용: {news_content}")
            print("-" * 100)
            print('')

            data_list.append([formatted_date, news_title, news_content])

            time.sleep(5)

        except AttributeError as e:
            print(f'에러 발생: {e}')
            print(f"{i}번째 기사를 처리하는 중 오류가 발생했습니다. 다음 기사로 이동합니다.")

    
    df = pd.DataFrame(data_list, columns=['Date', 'Title', 'Contents'])

   
    csv_file_path = f'/content/drive/MyDrive/Colab Notebooks/the hill/news_page_{page_number}.csv'
    df.to_csv(csv_file_path, index=False, encoding='utf-8')

    print(f'데이터가 {csv_file_path}에 저장되었습니다.')
    print('==============================================저장완료==============================================')

    # 1분 동안 대기
    print("Sleeping for 1 minute...")
    time.sleep(1 * 60)

"""#the hill txt파일 저장"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import os
import re

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}


data_list = []

# 625페이지부터 1페이지까지 역순으로 크롤링
for page_number in range(508, 499, -1):
    print(page_number)
    URL = f'https://thehill.com/page/{page_number}/?s=interest+rate&submit=Search&order=desc&orderby=date'
    response = requests.get(URL, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    news = soup.select('div.featured-cards__full__content h1 a')

    # 기사 크롤링
    for i in range(16, 0, -1):
        try:
            current_news_url = news[i].get('href')
            current_news_response = requests.get(current_news_url, headers=headers)
            current_news_soup = BeautifulSoup(current_news_response.text, 'html.parser')

            # 뉴스 제목 추출
            news_title = current_news_soup.select_one('h1').text.strip()

            # 기사 날짜 추출
            date = current_news_soup.find('section', class_='article__header').find('span').text.strip()
            date_string = date
            date_pattern = r'(\d{2}/\d{2}/\d{2})'
            match = re.search(date_pattern, date_string)

            formatted_date = None  # 기본값 설정

            if match:
                extracted_date = match.group(1)
                formatted_date = datetime.strptime(extracted_date, "%m/%d/%y").strftime("%Y-%m-%d")
                print("Date:", formatted_date)

            # 뉴스 기사 내용 추출
            news_content = current_news_soup.select_one('div.article__text').text.strip()


            print('')
            if formatted_date:
                print(f"날짜: {formatted_date}")
            print(f"제목: {news_title}")
            print(f"내용: {news_content}")
            print("-" * 100)
            print('')

            data_list.append(f"날짜: {formatted_date}\n제목: {news_title}\n내용: {news_content}\n{'=' * 100}\n")

            time.sleep(5)

        except AttributeError as e:
            print(f'에러 발생: {e}')
            print(f"{i}번째 기사를 처리하는 중 오류가 발생했습니다. 다음 기사로 이동합니다.")


    text_file_path = f'/content/drive/MyDrive/Colab Notebooks/the hill/news_page_{page_number}.txt'
    with open(text_file_path, 'w', encoding='utf-8') as text_file:
        text_file.write("\n".join(data_list))

    print(f'데이터가 {text_file_path}에 저장되었습니다.')
    print('==============================================저장완료==============================================')

    # 1분 동안 대기
    print("Sleeping for 1 minute...")
    time.sleep(1 * 60)